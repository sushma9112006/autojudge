{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0841f8c3-3405-4b25-bbb7-5632ab5508db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description           0\n",
       "input_description     0\n",
       "output_description    0\n",
       "problem_class         0\n",
       "problem_score         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_to_dense(x):\n",
    "    return x.toarray()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer,StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, VotingClassifier,RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score,precision_recall_curve,mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import clone\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#LOAD\n",
    "url = \"https://raw.githubusercontent.com/AREEG94FAHAD/TaskComplexityEval-24/refs/heads/main/problems_data.jsonl\"\n",
    "df = pd.read_json(url, lines=True)\n",
    "df = df.drop(columns=['sample_io', 'url', 'title'], errors='ignore')\n",
    "#df.sample(5)\n",
    "df.isnull().sum()\n",
    "#plt.bar(df['problem_class'].value_counts().index, df['problem_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb76524e-a478-4589-a555-ee3f0aa801c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen HARD threshold (CV): 0.5384\n",
      "Accuracy: 0.5176184690157959\n",
      "[[ 80  20  53]\n",
      " [ 39 210 140]\n",
      " [ 41 104 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        easy       0.50      0.52      0.51       153\n",
      "        hard       0.63      0.54      0.58       389\n",
      "      medium       0.41      0.48      0.45       281\n",
      "\n",
      "    accuracy                           0.52       823\n",
      "   macro avg       0.51      0.52      0.51       823\n",
      "weighted avg       0.53      0.52      0.52       823\n",
      "\n",
      "Mean Absolute Error (MAE): 1.6623\n",
      "Root Mean Squared Error (RMSE): 2.0040\n"
     ]
    }
   ],
   "source": [
    "targets = [\"problem_score\", \"problem_class\"]\n",
    "text_cols = [c for c in df.columns if c not in targets]\n",
    "\n",
    "df[\"full_text\"] = df[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
    "#df = df.drop(columns=text_cols)\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"≤\", \"<=\").replace(\"≥\", \">=\").replace(\"≠\", \"!=\")\n",
    "    s = re.sub(r\"[→⇒]\", \"->\", s)\n",
    "    s = re.sub(r\"(\\d+)\\s*\\^\\s*(\\d+)\", r\"\\1^\\2\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    sentences = re.split(r\"[.!?]\", s)\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if sent and sent not in seen:\n",
    "            seen.add(sent)\n",
    "            deduped.append(sent)\n",
    "\n",
    "    s = \". \".join(deduped)\n",
    "\n",
    "    return s\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(clean_text)\n",
    "dups = df[df.duplicated(subset=['full_text'], keep=False)].copy()\n",
    "conflict_mask = dups.groupby('full_text')['problem_class'].transform('nunique') > 1\n",
    "conflicting_dups = dups[conflict_mask].sort_values(by='full_text')\n",
    "consistent_dups = dups[~conflict_mask].sort_values(by='full_text')\n",
    "#print(f\"Rows with CONFLICTING labels: {len(conflicting_dups)}\")\n",
    "#print(f\"Rows with CONSISTENT labels: {len(consistent_dups)}\")\n",
    "df = df.drop_duplicates(subset=[\"full_text\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#FEATUREENGINEERING\n",
    "KEYWORDS = [ \"dp\",\"greedy\",\"binary search\",\"two pointers\",\"sliding window\", \"recursion\",\"backtracking\",\"divide and conquer\",\"bitmask\", \"array\",\"string\",\"stack\",\"queue\",\"heap\",\"priority queue\", \"hashmap\",\"set\",\"tree\",\"binary tree\",\"bst\",\"segment tree\", \"fenwick\",\"trie\",\"graph\",\"dag\",\"linked list\",\"disjoint set\",\"union find\", \"bfs\",\"dfs\",\"shortest path\",\"dijkstra\",\"bellman ford\",\"floyd\", \"mst\",\"kruskal\",\"prim\",\"topological\",\"cycle\",\"bipartite\", \"modulo\",\"gcd\",\"lcm\",\"prime\",\"sieve\",\"combinatorics\",\"probability\", \"matrix\",\"prefix sum\",\"xor\",\"bitwise\", \"substring\",\"subsequence\",\"palindrome\",\"z algorithm\",\"kmp\",\"hashing\", \"simulation\",\"implementation\",\"geometry\",\"game theory\"]\n",
    "\n",
    "def numeric_features(X):\n",
    "    # ALWAYS force 1D text\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        text = X.iloc[:, 0]\n",
    "    elif isinstance(X, pd.Series):\n",
    "        text = X\n",
    "    else:\n",
    "        text = pd.Series(X)\n",
    "\n",
    "    text = text.fillna(\"\").astype(str)\n",
    "\n",
    "    length = text.str.len().values.reshape(-1, 1)\n",
    "    math_symbols = text.str.count(r\"[=<>+\\-*/%^]\").values.reshape(-1, 1)\n",
    "\n",
    "    keyword_counts = np.column_stack([\n",
    "        text.str.contains(rf\"\\b{k}\\b\", case=False, regex=True).astype(int)\n",
    "        for k in KEYWORDS\n",
    "    ])\n",
    "\n",
    "    return np.hstack([length, math_symbols, keyword_counts])\n",
    "\n",
    "num_feat = FunctionTransformer(numeric_features, validate=False)\n",
    "\n",
    "features = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True\n",
    "    ), \"full_text\"),\n",
    "    (\"numeric\", Pipeline([\n",
    "        (\"extract\", num_feat),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ]), \"full_text\")\n",
    "])\n",
    "X = df[[\"full_text\"]]\n",
    "y = df[\"problem_class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "##########2-PASS MODEL: I first categorised data into hard vs not-hard , then from the not-hard i again labelled as medium , easy\n",
    "\n",
    "# Binary target\n",
    "y_train_s1 = (y_train == \"hard\").astype(int)\n",
    "\n",
    "stage1 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=18,\n",
    "        min_samples_leaf=4,\n",
    "        min_samples_split=10,\n",
    "        class_weight={0: 1.0, 1: 1.32},\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "cv_probs = cross_val_predict(\n",
    "    stage1,\n",
    "    X_train,\n",
    "    y_train_s1,\n",
    "    cv=5,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1\n",
    ")[:, 1]\n",
    "best_acc = -1\n",
    "best_threshold = 0.5\n",
    "\n",
    "for t in np.linspace(0.3, 0.7, 100):\n",
    "    preds = (cv_probs >= t).astype(int)\n",
    "    acc = accuracy_score(y_train_s1, preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_threshold = t\n",
    "\n",
    "HARD_T = best_threshold\n",
    "print(f\"Frozen HARD threshold (CV): {HARD_T:.4f}\")\n",
    "stage1.fit(X_train, y_train_s1)\n",
    "hard_proba = stage1.predict_proba(X_test)[:, 1]\n",
    "hard_pred = hard_proba >= HARD_T\n",
    "mask_train_s2 = y_train != \"hard\"\n",
    "mask_test_s2  = ~hard_pred\n",
    "\n",
    "X_train_s2 = X_train.loc[mask_train_s2]\n",
    "y_train_s2 = y_train.loc[mask_train_s2]\n",
    "\n",
    "X_test_s2 = X_test.loc[mask_test_s2]\n",
    "stage2 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", LinearSVC(\n",
    "        C=0.732,\n",
    "        #class_weight=\"balanced\",\n",
    "        class_weight={\n",
    "        \"easy\": 1.3,\n",
    "        \"medium\": 1.0\n",
    "    },\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "stage2.fit(X_train_s2, y_train_s2)\n",
    "stage2_pred = stage2.predict(X_test_s2)\n",
    "final_pred = np.array([\"hard\"] * len(X_test), dtype=object)\n",
    "final_pred[mask_test_s2] = stage2_pred\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, final_pred))\n",
    "print(confusion_matrix(y_test, final_pred))\n",
    "print(classification_report(y_test, final_pred))\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "X = df[[\"full_text\"]]\n",
    "y = df[\"problem_score\"] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "reg_pipe = Pipeline([\n",
    "    (\"features\", clone(features)), \n",
    "    (\"to_dense\", FunctionTransformer(\n",
    "    sparse_to_dense,\n",
    "    accept_sparse=True\n",
    ")),\n",
    "    (\"reg\", HistGradientBoostingRegressor(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=300,\n",
    "        random_state=42,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg_pipe.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "#rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2cbf64-d1fd-4df8-a931-a2e356037f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['score_regressor.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(stage1, \"stage1_hard_classifier.pkl\")\n",
    "joblib.dump(HARD_T, \"hard_threshold.pkl\")\n",
    "joblib.dump(stage2, \"stage2_easy_medium.pkl\")\n",
    "joblib.dump(reg_pipe, \"score_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0730ae-de1c-4eba-a28e-2ae4bee616fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
