{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0841f8c3-3405-4b25-bbb7-5632ab5508db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description           0\n",
       "input_description     0\n",
       "output_description    0\n",
       "problem_class         0\n",
       "problem_score         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_to_dense(x):\n",
    "    return x.toarray()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer,StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, VotingClassifier,RandomForestRegressor,HistGradientBoostingRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score,precision_recall_curve,mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import clone\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#LOAD\n",
    "url = \"https://raw.githubusercontent.com/AREEG94FAHAD/TaskComplexityEval-24/refs/heads/main/problems_data.jsonl\"\n",
    "df = pd.read_json(url, lines=True)\n",
    "df = df.drop(columns=['sample_io', 'url', 'title'], errors='ignore')\n",
    "#df.sample(5)\n",
    "df.isnull().sum()\n",
    "#plt.bar(df['problem_class'].value_counts().index, df['problem_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fec5aa0-6186-48fd-b770-47ea2e87f34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen HARD threshold (CV): 0.5384\n",
      "Accuracy: 0.5176184690157959\n",
      "[[ 80  20  53]\n",
      " [ 39 210 140]\n",
      " [ 41 104 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        easy       0.50      0.52      0.51       153\n",
      "        hard       0.63      0.54      0.58       389\n",
      "      medium       0.41      0.48      0.45       281\n",
      "\n",
      "    accuracy                           0.52       823\n",
      "   macro avg       0.51      0.52      0.51       823\n",
      "weighted avg       0.53      0.52      0.52       823\n",
      "\n",
      "Raw Score MAE (before calibration): 1.6692\n",
      "Raw Score RMSE (before calibration): 2.0041\n",
      "Calibrated Score MAE (after calibration): 1.6532\n",
      "Calibrated Score RMSE (after calibration): 2.0700\n"
     ]
    }
   ],
   "source": [
    "targets = [\"problem_score\", \"problem_class\"]\n",
    "text_cols = [c for c in df.columns if c not in targets]\n",
    "df[\"full_text\"] = df[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"≤\", \"<=\").replace(\"≥\", \">=\").replace(\"≠\", \"!=\")\n",
    "    s = re.sub(r\"[→⇒]\", \"->\", s)\n",
    "    s = re.sub(r\"(\\d+)\\s*\\^\\s*(\\d+)\", r\"\\1^\\2\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    sentences = re.split(r\"[.!?]\", s)\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if sent and sent not in seen:\n",
    "            seen.add(sent)\n",
    "            deduped.append(sent)\n",
    "    s = \". \".join(deduped)\n",
    "    return s\n",
    "\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(clean_text)\n",
    "df = df.drop_duplicates(subset=[\"full_text\"]).reset_index(drop=True)\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "KEYWORDS = [ \"dp\",\"greedy\",\"binary search\",\"two pointers\",\"sliding window\", \"recursion\",\"backtracking\",\"divide and conquer\",\"bitmask\", \"array\",\"string\",\"stack\",\"queue\",\"heap\",\"priority queue\", \"hashmap\",\"set\",\"tree\",\"binary tree\",\"bst\",\"segment tree\", \"fenwick\",\"trie\",\"graph\",\"dag\",\"linked list\",\"disjoint set\",\"union find\", \"bfs\",\"dfs\",\"shortest path\",\"dijkstra\",\"bellman ford\",\"floyd\", \"mst\",\"kruskal\",\"prim\",\"topological\",\"cycle\",\"bipartite\", \"modulo\",\"gcd\",\"lcm\",\"prime\",\"sieve\",\"combinatorics\",\"probability\", \"matrix\",\"prefix sum\",\"xor\",\"bitwise\", \"substring\",\"subsequence\",\"palindrome\",\"z algorithm\",\"kmp\",\"hashing\", \"simulation\",\"implementation\",\"geometry\",\"game theory\"]\n",
    "\n",
    "def numeric_features(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        text = X.iloc[:, 0]\n",
    "    elif isinstance(X, pd.Series):\n",
    "        text = X\n",
    "    else:\n",
    "        text = pd.Series(X)\n",
    "    text = text.fillna(\"\").astype(str)\n",
    "    length = text.str.len().values.reshape(-1, 1)\n",
    "    math_symbols = text.str.count(r\"[=<>+\\-*/%^]\").values.reshape(-1, 1)\n",
    "    keyword_counts = np.column_stack([\n",
    "        text.str.contains(rf\"\\b{k}\\b\", case=False, regex=True).astype(int)\n",
    "        for k in KEYWORDS\n",
    "    ])\n",
    "    return np.hstack([length, math_symbols, keyword_counts])\n",
    "\n",
    "num_feat = FunctionTransformer(numeric_features, validate=False)\n",
    "\n",
    "features = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True\n",
    "    ), \"full_text\"),\n",
    "    (\"numeric\", Pipeline([\n",
    "        (\"extract\", num_feat),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ]), \"full_text\")\n",
    "])\n",
    "\n",
    "# SINGLE CONSISTENT SPLIT FOR EVERYTHING\n",
    "X_all = df[[\"full_text\"]]\n",
    "y_class = df[\"problem_class\"]\n",
    "y_score = df[\"problem_score\"]\n",
    "\n",
    "X_train, X_test, y_train_class, y_test_class, y_train_score, y_test_score = train_test_split(\n",
    "    X_all, y_class, y_score,\n",
    "    test_size=0.2,\n",
    "    stratify=y_class,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# CLASSIFICATION (2-STAGE)\n",
    "y_train_s1 = (y_train_class == \"hard\").astype(int)\n",
    "\n",
    "stage1 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=18,\n",
    "        min_samples_leaf=4,\n",
    "        min_samples_split=10,\n",
    "        class_weight={0: 1.0, 1: 1.32},\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "cv_probs = cross_val_predict(\n",
    "    stage1,\n",
    "    X_train,\n",
    "    y_train_s1,\n",
    "    cv=5,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1\n",
    ")[:, 1]\n",
    "\n",
    "best_acc = -1\n",
    "best_threshold = 0.5\n",
    "for t in np.linspace(0.3, 0.7, 100):\n",
    "    preds = (cv_probs >= t).astype(int)\n",
    "    acc = accuracy_score(y_train_s1, preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_threshold = t\n",
    "\n",
    "HARD_T = best_threshold\n",
    "print(f\"Frozen HARD threshold (CV): {HARD_T:.4f}\")\n",
    "\n",
    "stage1.fit(X_train, y_train_s1)\n",
    "hard_proba_test = stage1.predict_proba(X_test)[:, 1]\n",
    "hard_pred_test = hard_proba_test >= HARD_T\n",
    "\n",
    "mask_train_s2 = y_train_class != \"hard\"\n",
    "mask_test_s2  = ~hard_pred_test\n",
    "\n",
    "X_train_s2 = X_train.loc[mask_train_s2]\n",
    "y_train_s2 = y_train_class.loc[mask_train_s2]\n",
    "\n",
    "X_test_s2 = X_test.loc[mask_test_s2]\n",
    "\n",
    "stage2 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", LinearSVC(\n",
    "        C=0.732,\n",
    "        class_weight={\n",
    "            \"easy\": 1.3,\n",
    "            \"medium\": 1.0\n",
    "        },\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "stage2.fit(X_train_s2, y_train_s2)\n",
    "stage2_pred_test = stage2.predict(X_test_s2)\n",
    "\n",
    "final_pred_class = np.array([\"hard\"] * len(X_test), dtype=object)\n",
    "final_pred_class[mask_test_s2] = stage2_pred_test\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, final_pred_class))\n",
    "print(confusion_matrix(y_test_class, final_pred_class))\n",
    "print(classification_report(y_test_class, final_pred_class))\n",
    "\n",
    "def sparse_to_dense(x):\n",
    "    return x.toarray()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "regressors = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"to_dense\", FunctionTransformer(sparse_to_dense, accept_sparse=True)),\n",
    "    (\"reg\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "regressors.fit(X_train, y_train_score)\n",
    "hard_proba_test = stage1.predict_proba(X_test)[:, 1]\n",
    "hard_pred_test = hard_proba_test >= HARD_T\n",
    "mask_test_s2 = ~hard_pred_test\n",
    "\n",
    "final_classes_test = np.array([\"hard\"] * len(X_test), dtype=object)\n",
    "final_classes_test[mask_test_s2] = stage2.predict(X_test.loc[mask_test_s2])\n",
    "raw_scores_test = regressors.predict(X_test)\n",
    "mae_raw = mean_absolute_error(y_test_score, raw_scores_test)\n",
    "rmse_raw = np.sqrt(mean_squared_error(y_test_score, raw_scores_test))\n",
    "\n",
    "print(f\"Raw Score MAE (before calibration): {mae_raw:.4f}\")\n",
    "print(f\"Raw Score RMSE (before calibration): {rmse_raw:.4f}\")\n",
    "\n",
    "final_scores_test = raw_scores_test.copy()\n",
    "for cls in [\"easy\", \"medium\", \"hard\"]:\n",
    "    idx = np.where(final_classes_test == cls)[0]\n",
    "    if len(idx) > 0:\n",
    "        if cls == \"easy\":\n",
    "            final_scores_test[idx] = np.minimum(raw_scores_test[idx], 2.8)\n",
    "        elif cls == \"medium\":\n",
    "            final_scores_test[idx] = np.clip(raw_scores_test[idx], 2.9, 5.5)\n",
    "        elif cls == \"hard\":\n",
    "            final_scores_test[idx] = np.clip(raw_scores_test[idx], 5.6, 10.0)\n",
    "mae_cal = mean_absolute_error(y_test_score, final_scores_test)\n",
    "rmse_cal = np.sqrt(mean_squared_error(y_test_score, final_scores_test))\n",
    "print(f\"Calibrated Score MAE (after calibration): {mae_cal:.4f}\")\n",
    "print(f\"Calibrated Score RMSE (after calibration): {rmse_cal:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e861316-a311-445f-aa88-57e3005a8e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen HARD threshold (CV): 0.5384\n",
      "Accuracy: 0.5176184690157959\n",
      "[[ 80  20  53]\n",
      " [ 39 210 140]\n",
      " [ 41 104 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        easy       0.50      0.52      0.51       153\n",
      "        hard       0.63      0.54      0.58       389\n",
      "      medium       0.41      0.48      0.45       281\n",
      "\n",
      "    accuracy                           0.52       823\n",
      "   macro avg       0.51      0.52      0.51       823\n",
      "weighted avg       0.53      0.52      0.52       823\n",
      "\n",
      "Score MAE (test): 1.6532\n",
      "Score RMSE (test): 2.0700\n"
     ]
    }
   ],
   "source": [
    "targets = [\"problem_score\", \"problem_class\"]\n",
    "text_cols = [c for c in df.columns if c not in targets]\n",
    "df[\"full_text\"] = df[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"≤\", \"<=\").replace(\"≥\", \">=\").replace(\"≠\", \"!=\")\n",
    "    s = re.sub(r\"[→⇒]\", \"->\", s)\n",
    "    s = re.sub(r\"(\\d+)\\s*\\^\\s*(\\d+)\", r\"\\1^\\2\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    sentences = re.split(r\"[.!?]\", s)\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if sent and sent not in seen:\n",
    "            seen.add(sent)\n",
    "            deduped.append(sent)\n",
    "    s = \". \".join(deduped)\n",
    "    return s\n",
    "\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(clean_text)\n",
    "df = df.drop_duplicates(subset=[\"full_text\"]).reset_index(drop=True)\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "KEYWORDS = [ \"dp\",\"greedy\",\"binary search\",\"two pointers\",\"sliding window\", \"recursion\",\"backtracking\",\"divide and conquer\",\"bitmask\", \"array\",\"string\",\"stack\",\"queue\",\"heap\",\"priority queue\", \"hashmap\",\"set\",\"tree\",\"binary tree\",\"bst\",\"segment tree\", \"fenwick\",\"trie\",\"graph\",\"dag\",\"linked list\",\"disjoint set\",\"union find\", \"bfs\",\"dfs\",\"shortest path\",\"dijkstra\",\"bellman ford\",\"floyd\", \"mst\",\"kruskal\",\"prim\",\"topological\",\"cycle\",\"bipartite\", \"modulo\",\"gcd\",\"lcm\",\"prime\",\"sieve\",\"combinatorics\",\"probability\", \"matrix\",\"prefix sum\",\"xor\",\"bitwise\", \"substring\",\"subsequence\",\"palindrome\",\"z algorithm\",\"kmp\",\"hashing\", \"simulation\",\"implementation\",\"geometry\",\"game theory\"]\n",
    "\n",
    "def numeric_features(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        text = X.iloc[:, 0]\n",
    "    elif isinstance(X, pd.Series):\n",
    "        text = X\n",
    "    else:\n",
    "        text = pd.Series(X)\n",
    "    text = text.fillna(\"\").astype(str)\n",
    "    length = text.str.len().values.reshape(-1, 1)\n",
    "    math_symbols = text.str.count(r\"[=<>+\\-*/%^]\").values.reshape(-1, 1)\n",
    "    keyword_counts = np.column_stack([\n",
    "        text.str.contains(rf\"\\b{k}\\b\", case=False, regex=True).astype(int)\n",
    "        for k in KEYWORDS\n",
    "    ])\n",
    "    return np.hstack([length, math_symbols, keyword_counts])\n",
    "\n",
    "num_feat = FunctionTransformer(numeric_features, validate=False)\n",
    "\n",
    "features = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True\n",
    "    ), \"full_text\"),\n",
    "    (\"numeric\", Pipeline([\n",
    "        (\"extract\", num_feat),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ]), \"full_text\")\n",
    "])\n",
    "\n",
    "# SINGLE CONSISTENT SPLIT FOR EVERYTHING\n",
    "X_all = df[[\"full_text\"]]\n",
    "y_class = df[\"problem_class\"]\n",
    "y_score = df[\"problem_score\"]\n",
    "\n",
    "X_train, X_test, y_train_class, y_test_class, y_train_score, y_test_score = train_test_split(\n",
    "    X_all, y_class, y_score,\n",
    "    test_size=0.2,\n",
    "    stratify=y_class,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# CLASSIFICATION (2-STAGE)\n",
    "y_train_s1 = (y_train_class == \"hard\").astype(int)\n",
    "\n",
    "stage1 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=18,\n",
    "        min_samples_leaf=4,\n",
    "        min_samples_split=10,\n",
    "        class_weight={0: 1.0, 1: 1.32},\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "cv_probs = cross_val_predict(\n",
    "    stage1,\n",
    "    X_train,\n",
    "    y_train_s1,\n",
    "    cv=5,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1\n",
    ")[:, 1]\n",
    "\n",
    "best_acc = -1\n",
    "best_threshold = 0.5\n",
    "for t in np.linspace(0.3, 0.7, 100):\n",
    "    preds = (cv_probs >= t).astype(int)\n",
    "    acc = accuracy_score(y_train_s1, preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_threshold = t\n",
    "\n",
    "HARD_T = best_threshold\n",
    "print(f\"Frozen HARD threshold (CV): {HARD_T:.4f}\")\n",
    "\n",
    "stage1.fit(X_train, y_train_s1)\n",
    "hard_proba_test = stage1.predict_proba(X_test)[:, 1]\n",
    "hard_pred_test = hard_proba_test >= HARD_T\n",
    "\n",
    "mask_train_s2 = y_train_class != \"hard\"\n",
    "mask_test_s2  = ~hard_pred_test\n",
    "\n",
    "X_train_s2 = X_train.loc[mask_train_s2]\n",
    "y_train_s2 = y_train_class.loc[mask_train_s2]\n",
    "\n",
    "X_test_s2 = X_test.loc[mask_test_s2]\n",
    "\n",
    "stage2 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", LinearSVC(\n",
    "        C=0.732,\n",
    "        class_weight={\n",
    "            \"easy\": 1.3,\n",
    "            \"medium\": 1.0\n",
    "        },\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "stage2.fit(X_train_s2, y_train_s2)\n",
    "stage2_pred_test = stage2.predict(X_test_s2)\n",
    "\n",
    "final_pred_class = np.array([\"hard\"] * len(X_test), dtype=object)\n",
    "final_pred_class[mask_test_s2] = stage2_pred_test\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, final_pred_class))\n",
    "print(confusion_matrix(y_test_class, final_pred_class))\n",
    "print(classification_report(y_test_class, final_pred_class))\n",
    "\n",
    "def sparse_to_dense(x):\n",
    "    return x.toarray()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "regressors = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"to_dense\", FunctionTransformer(sparse_to_dense, accept_sparse=True)),\n",
    "    (\"reg\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "regressors.fit(X_train, y_train_score)\n",
    "hard_proba_test = stage1.predict_proba(X_test)[:, 1]\n",
    "hard_pred_test = hard_proba_test >= HARD_T\n",
    "mask_test_s2 = ~hard_pred_test\n",
    "\n",
    "final_classes_test = np.array([\"hard\"] * len(X_test), dtype=object)\n",
    "final_classes_test[mask_test_s2] = stage2.predict(X_test.loc[mask_test_s2])\n",
    "raw_scores_test = regressors.predict(X_test)\n",
    "\n",
    "final_scores_test = raw_scores_test.copy()\n",
    "for cls in [\"easy\", \"medium\", \"hard\"]:\n",
    "    idx = np.where(final_classes_test == cls)[0]\n",
    "    if len(idx) > 0:\n",
    "        if cls == \"easy\":\n",
    "            final_scores_test[idx] = np.minimum(raw_scores_test[idx], 2.8)\n",
    "        elif cls == \"medium\":\n",
    "            final_scores_test[idx] = np.clip(raw_scores_test[idx], 2.9, 5.5)\n",
    "        elif cls == \"hard\":\n",
    "            final_scores_test[idx] = np.clip(raw_scores_test[idx], 5.6, 10.0)\n",
    "\n",
    "mae = mean_absolute_error(y_test_score, final_scores_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_score, final_scores_test))\n",
    "print(f\"Score MAE (test): {mae:.4f}\")\n",
    "print(f\"Score RMSE (test): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2cbf64-d1fd-4df8-a931-a2e356037f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['score_regressors.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(stage1, \"stage1_hard_classifier.pkl\")\n",
    "joblib.dump(HARD_T, \"hard_threshold.pkl\")\n",
    "joblib.dump(stage2, \"stage2_easy_medium.pkl\")\n",
    "joblib.dump(regressors, \"score_regressors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d935961-c83c-4ac4-acff-198dbff439e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mn</th>\n",
       "      <th>mx</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>length_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem_class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>766</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.970888</td>\n",
       "      <td>0.433289</td>\n",
       "      <td>1193.221932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>1940</td>\n",
       "      <td>5.5</td>\n",
       "      <td>9.7</td>\n",
       "      <td>7.071443</td>\n",
       "      <td>1.049919</td>\n",
       "      <td>1627.293299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1405</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.125836</td>\n",
       "      <td>0.774216</td>\n",
       "      <td>1495.689680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count   mn   mx      mean       std  length_mean\n",
       "problem_class                                                  \n",
       "easy             766  1.1  2.8  1.970888  0.433289  1193.221932\n",
       "hard            1940  5.5  9.7  7.071443  1.049919  1627.293299\n",
       "medium          1405  2.8  5.5  4.125836  0.774216  1495.689680"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"problem_class\").agg(\n",
    "    count=(\"problem_score\", \"count\"),\n",
    "    mn=(\"problem_score\", \"min\"),\n",
    "    mx=(\"problem_score\", \"max\"),\n",
    "    mean=(\"problem_score\", \"mean\"),\n",
    "    std=(\"problem_score\", \"std\"),\n",
    "    length_mean=(\"full_text\", lambda x: x.str.len().mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4e169-4f54-4c8f-b92b-f78d67a19aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba50ae5-0c65-4817-aff2-fd33ba3c5204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
