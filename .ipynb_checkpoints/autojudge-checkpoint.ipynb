{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0841f8c3-3405-4b25-bbb7-5632ab5508db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_dense(x):\n",
    "    return x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb76524e-a478-4589-a555-ee3f0aa801c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen HARD threshold (CV): 0.5424\n",
      "Accuracy: 0.5151883353584447\n",
      "[[ 90  14  49]\n",
      " [ 52 209 128]\n",
      " [ 62  94 125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        easy       0.44      0.59      0.50       153\n",
      "        hard       0.66      0.54      0.59       389\n",
      "      medium       0.41      0.44      0.43       281\n",
      "\n",
      "    accuracy                           0.52       823\n",
      "   macro avg       0.50      0.52      0.51       823\n",
      "weighted avg       0.53      0.52      0.52       823\n",
      "\n",
      "Mean Absolute Error (MAE): 1.6684\n",
      "Root Mean Squared Error (RMSE): 1.9961\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, VotingClassifier,RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score,precision_recall_curve,mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#LOAD\n",
    "url = \"https://raw.githubusercontent.com/AREEG94FAHAD/TaskComplexityEval-24/refs/heads/main/problems_data.jsonl\"\n",
    "df = pd.read_json(url, lines=True)\n",
    "df = df.drop(columns=['sample_io', 'url', 'title'], errors='ignore')\n",
    "#df.sample(5)\n",
    "#df.isnull().sum()\n",
    "#plt.bar(df['problem_class'].value_counts().index, df['problem_class'].value_counts())\n",
    "\n",
    "targets = [\"problem_score\", \"problem_class\"]\n",
    "text_cols = [c for c in df.columns if c not in targets]\n",
    "\n",
    "df[\"full_text\"] = df[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
    "#df = df.drop(columns=text_cols)\n",
    "\n",
    "#FEATUREENGINEERING\n",
    "KEYWORDS = [ \"dp\",\"greedy\",\"binary search\",\"two pointers\",\"sliding window\", \"recursion\",\"backtracking\",\"divide and conquer\",\"bitmask\", \"array\",\"string\",\"stack\",\"queue\",\"heap\",\"priority queue\", \"hashmap\",\"set\",\"tree\",\"binary tree\",\"bst\",\"segment tree\", \"fenwick\",\"trie\",\"graph\",\"dag\",\"linked list\",\"disjoint set\",\"union find\", \"bfs\",\"dfs\",\"shortest path\",\"dijkstra\",\"bellman ford\",\"floyd\", \"mst\",\"kruskal\",\"prim\",\"topological\",\"cycle\",\"bipartite\", \"modulo\",\"gcd\",\"lcm\",\"prime\",\"sieve\",\"combinatorics\",\"probability\", \"matrix\",\"prefix sum\",\"xor\",\"bitwise\", \"substring\",\"subsequence\",\"palindrome\",\"z algorithm\",\"kmp\",\"hashing\", \"simulation\",\"implementation\",\"geometry\",\"game theory\"]\n",
    "\n",
    "def numeric_features(X):\n",
    "    # ALWAYS force 1D text\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        text = X.iloc[:, 0]\n",
    "    elif isinstance(X, pd.Series):\n",
    "        text = X\n",
    "    else:\n",
    "        text = pd.Series(X)\n",
    "\n",
    "    text = text.fillna(\"\").astype(str)\n",
    "\n",
    "    length = text.str.len().values.reshape(-1, 1)\n",
    "    math_symbols = text.str.count(r\"[=<>+\\-*/%^]\").values.reshape(-1, 1)\n",
    "\n",
    "    keyword_counts = np.column_stack([\n",
    "        text.str.contains(rf\"\\b{k}\\b\", case=False, regex=True).astype(int)\n",
    "        for k in KEYWORDS\n",
    "    ])\n",
    "\n",
    "    return np.hstack([length, math_symbols, keyword_counts])\n",
    "\n",
    "num_feat = FunctionTransformer(numeric_features, validate=False)\n",
    "\n",
    "features = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True\n",
    "    ), \"full_text\"),\n",
    "    (\"numeric\", Pipeline([\n",
    "        (\"extract\", num_feat),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ]), \"full_text\")\n",
    "])\n",
    "X = df[[\"full_text\"]]\n",
    "y = df[\"problem_class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "##########2-PASS MODEL: I first categorised data into hard vs not-hard , then from the not-hard i again labelled as medium , easy\n",
    "\n",
    "# Binary target\n",
    "y_train_s1 = (y_train == \"hard\").astype(int)\n",
    "\n",
    "stage1 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=18,\n",
    "        min_samples_leaf=4,\n",
    "        min_samples_split=10,\n",
    "        class_weight={0: 1.0, 1: 1.32},\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "cv_probs = cross_val_predict(\n",
    "    stage1,\n",
    "    X_train,\n",
    "    y_train_s1,\n",
    "    cv=5,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1\n",
    ")[:, 1]\n",
    "best_acc = -1\n",
    "best_threshold = 0.5\n",
    "\n",
    "for t in np.linspace(0.3, 0.7, 100):\n",
    "    preds = (cv_probs >= t).astype(int)\n",
    "    acc = accuracy_score(y_train_s1, preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_threshold = t\n",
    "\n",
    "HARD_T = best_threshold\n",
    "print(f\"Frozen HARD threshold (CV): {HARD_T:.4f}\")\n",
    "stage1.fit(X_train, y_train_s1)\n",
    "hard_proba = stage1.predict_proba(X_test)[:, 1]\n",
    "hard_pred = hard_proba >= HARD_T\n",
    "mask_train_s2 = y_train != \"hard\"\n",
    "mask_test_s2  = ~hard_pred\n",
    "\n",
    "X_train_s2 = X_train.loc[mask_train_s2]\n",
    "y_train_s2 = y_train.loc[mask_train_s2]\n",
    "\n",
    "X_test_s2 = X_test.loc[mask_test_s2]\n",
    "stage2 = Pipeline([\n",
    "    (\"features\", clone(features)),\n",
    "    (\"clf\", LinearSVC(\n",
    "        C=0.73,\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "stage2.fit(X_train_s2, y_train_s2)\n",
    "stage2_pred = stage2.predict(X_test_s2)\n",
    "final_pred = np.array([\"hard\"] * len(X_test), dtype=object)\n",
    "final_pred[mask_test_s2] = stage2_pred\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, final_pred))\n",
    "print(confusion_matrix(y_test, final_pred))\n",
    "print(classification_report(y_test, final_pred))\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "X = df[[\"full_text\"]]\n",
    "y = df[\"problem_score\"] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "reg_pipe = Pipeline([\n",
    "    (\"features\", clone(features)), \n",
    "    (\"to_dense\", FunctionTransformer(\n",
    "    sparse_to_dense,\n",
    "    accept_sparse=True\n",
    ")),\n",
    "    (\"reg\", HistGradientBoostingRegressor(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=300,\n",
    "        random_state=42,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg_pipe.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "#rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2cbf64-d1fd-4df8-a931-a2e356037f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['score_regressor.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(stage1, \"stage1_hard_classifier.pkl\")\n",
    "joblib.dump(HARD_T, \"hard_threshold.pkl\")\n",
    "joblib.dump(stage2, \"stage2_easy_medium.pkl\")\n",
    "joblib.dump(reg_pipe, \"score_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf241558-ddff-406e-9728-901b686a0338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
